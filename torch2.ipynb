{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c367ee94",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch 2\n",
    "\n",
    "#### Object-Oriented Programming\n",
    "OOP-defined objects in PyTorch include `Dataset`s and Models (`nn.Module`, PyTorch's base class for neural networks).\n",
    "\n",
    "For a `Dataset`, common methods of note include:\n",
    "- `def __init__(self, ...)` which is called when the object is created.\n",
    "    - Note that for a subclass of PyTorch's `Dataset`, `super().__init__()` is needed to call the constructor of `Dataset` to ensure the subclass retains functionality despite overriding the superclass constructor.\n",
    "- `def __len__(self)` which returns the size of the `Dataset` (number of entries).\n",
    "- `def __getitem__(self, idx):` which extracts features and label for a single sample at index `idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34390b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.data = df.to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx, :-1] # gets the ith row's features (all columns except the last)\n",
    "        label = self.data[idx, -1] # gets the ith row's features (the last column)\n",
    "        return features, label\n",
    "\n",
    "# dataset_train = MyDataset(\"dataset.csv\")\n",
    "# dataloader_train = DataLoader(dataset_train, batch_size=2, shuffle=True)\n",
    "# features, labels = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea4dc0",
   "metadata": {},
   "source": [
    "For a Model, common methods of note include:\n",
    "- `def __init__(self, ...)` which is called when the object is created.\n",
    "    - Note that for a subclass of PyTorch's `nn.Module`, `super().__init__()` is needed to call the constructor of `nn.Module` to ensure the subclass retains functionality despite overriding the superclass constructor.\n",
    "- `def forward(self, x)` which runs a forward pass from input to output. In particular, each layer's output is wrapped in the activation function that succeeds it.\n",
    "\n",
    "Note that `nn.functional.relu()` and `nn.functional.sigmoid()` are inline activation functions that behave identically to `nn.ReLU` and `nn.Sigmoid()` respectively; use the former in `forward()` for inline, stateless activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2a58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(9, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(9, 16) # fc = shorthand for fully-connected layer\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "mymodel = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7071f",
   "metadata": {},
   "source": [
    "#### Optimizers, training, and evaluation\n",
    "\n",
    "Recall the PyTorch training loop:\n",
    "- creating a model\n",
    "- choosing a loss function\n",
    "- defining a dataset\n",
    "- setting an optimizer (optimized gradient descent function)\n",
    "- and running the training loop (calculating loss via a forward pass, computing gradients via backpropagation, and updating model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704d2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# binary cross entropy error; CSE specifically for bianry classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(mymodel.parameters(), lr=0.01)\n",
    "\n",
    "X = torch.randn((10, 9)) # dummy input\n",
    "y = nn.init.uniform_(torch.randn((10, 1))) # dummy target\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader_train = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "# Loop over the number of epochs and then the dataloader\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in dataloader_train:\n",
    "        optimizer.zero_grad() # Set the gradients to zero\n",
    "        outputs = mymodel(features) # Run a forward pass\n",
    "        loss = criterion(outputs, labels.view(-1,1)) # Compute loss\n",
    "        loss.backward() # Update gradients\n",
    "        optimizer.step() # Update parameters by descending gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6717e",
   "metadata": {},
   "source": [
    "#### Other optimizers\n",
    "\n",
    "Recall that the Stochastic Gradient Descent (SGD) optimizer bases the magnitude of its parameter updates on learning rate.\n",
    "- This is simple and computationally efficient for basic models but rarely used in practice in lieu of more sophisticated methods.\n",
    "\n",
    "Using the same learning rate for each parameter cannot be optimal, so the Adagrad optimizer adapts learning rate for each parameter by scaling it proportionally to the frequency at which the parameter is updated (frequent = greater learning rate; infrequent = smaller learning rate).\n",
    "- However, Adagrad tends to decrease the learning rate too steeply.\n",
    "\n",
    "Root Mean Square Propagation (RMSprop) addresses Adagrad's steep learning rate decay by adapting the learning rate per parameter based on the size of its previous gradients.\n",
    "\n",
    "The most versatile and widely used optimizer is the default go-to Adaptive Moment Estimation (Adam), which combines RMSprop with the concept of momentum (the weighted average of past gradients weighed toward the most recent ones). Basing the update on both gradient size and momentum accelerates training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6464e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = optim.SGD(mymodel.parameters(), lr=0.01)\n",
    "optimizer_adagrad = optim.Adagrad(mymodel.parameters(), lr=0.01)\n",
    "optimizer_rmsprop = optim.RMSprop(mymodel.parameters(), lr=0.01)\n",
    "optimizer_adam = optim.Adam(mymodel.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66eb27",
   "metadata": {},
   "source": [
    "#### Accuracy with torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a52ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "# acc = Accuracy(task=\"binary\")\n",
    "\n",
    "mymodel.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        outputs = mymodel(features)\n",
    "        predictions = (outputs >= 0.5).float() # for binary prediction (0, 1)\n",
    "        # acc(predictions, labels.view(-1, 1))\n",
    "\n",
    "# accuracy = acc.compute()\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2af52b",
   "metadata": {},
   "source": [
    "#### Vanishing and exploding gradients\n",
    "\n",
    "Neural networks may suffer from gradient instability (change in gradient updates from output to input layer) during training:\n",
    "- Vanishing gradients are when gradients shrink through backpropagation, which fails to update earlier layers' parameters efficiently\n",
    "- Exploding gradients are when gradients inflate toward huge parameter updates and divergent training\n",
    "\n",
    "A three-step solution to addressing gradient instability: proper weights initialization, good activations, and batch normalization.\n",
    "\n",
    "#### Weights initialization\n",
    "- Per the latest research, good weights initialization should ensure that the variance of the layer's inputs is similar to its outputs and the variance of its gradients is similar before and after passing through the layer. Achieving this is different for each activation function:\n",
    "    - For Rectified Linear Unit (ReLU) and similar activations, use He/Kaiming initialization. Call `kaiming_uniform_()` from `torch.nn.init` and pass the layer's `weight` attribute to ensure the desired variance properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bb5739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0943, -0.7111, -0.2765, -0.3434,  0.3502,  0.5823, -0.7252,  0.6005],\n",
      "        [ 0.7573,  0.5972,  0.1416, -0.2971, -0.2139,  0.1173, -0.5487, -0.7062]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "layer = nn.Linear(8, 2)\n",
    "\n",
    "init.kaiming_uniform_(layer.weight)\n",
    "print(layer.weight)\n",
    "\n",
    "# @@@ Implementation of He/Kaiming initialization @@@\n",
    "class KaimingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(9, 16) # fc = shorthand for fully-connected layer\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(\n",
    "            self.fc3.weight,\n",
    "            nonlinearity=\"sigmoid\"\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca2b0f",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "ReLU is available as the default `nn.functional.relu()`, but its zeroing of negative inputs suffers from the dying neuron problem.\n",
    "\n",
    "An improvement is the Exponential Linear Unit (ELU) function (supplied in PyTorch by `nn.functional.elu()), which merely approaches 0 with negative inputs; because of its nonzero gradients it avoids the dying neurons problem, and its average output being near zero makes it less prone to vanishing gradients.\n",
    "\n",
    "#### Batch normalization\n",
    "\n",
    "Batch normalization is an operation applied after a layer that first normalizes the layer's outputs (subtracting the mean, then dividing by the standard deviation to ensure a roughly normal output distribution), then scales and shifts the normalized outputs using learned model parameters. This enables the model to learn the optimal distribution to each layer before it is applied, speeding up the loss decrease and increasing resilience against unstable gradient issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c27e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ Implementation of batch normalization@@@\n",
    "class KaimingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(9, 16) # fc = shorthand for fully-connected layer\n",
    "        self.bn1 = nn.BatchNorm1d(16) # bn = batch normalization\n",
    "        # ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "        return x\n",
    "    \n",
    "    # Whether to place the activation function after or before batch normalization may depend on the model and dataset.\n",
    "    # Originally it was prescribed to apply BN before, but as of recent research\n",
    "    # applying after can normalize the activation toward better statistics for the next layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

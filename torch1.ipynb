{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811915f",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch\n",
    "Tensors are similar to NumPy arrays but have unique features. Convert a Python list into a Torch tensor with `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7af35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temperatures = [[10, 20], [30, 40], [50, 60]]\n",
    "\n",
    "temperature_tensor = torch.tensor(temperatures)\n",
    "print(temperature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1095cd",
   "metadata": {},
   "source": [
    "Tensors have a `shape` and `dtype`, and can be added elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums: tensor([[20, 40],\n",
      "        [40, 60],\n",
      "        [60, 80]])\n",
      "Sums shape: torch.Size([3, 2])\n",
      "Sums shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "addend_tensor = torch.tensor([[10, 20], [10, 20], [10, 20]])\n",
    "sums_tensor = temperature_tensor + addend_tensor\n",
    "print(\"Sums:\", sums_tensor)\n",
    "print(\"Sums shape:\", sums_tensor.shape)\n",
    "print(\"Sums shape:\", sums_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797e9f",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "`Linear` from `torch.nn` takes a tensor as input and outputs a tensor whose sizes correspond to `in_features` and `out_features`. The weights and biases involved in the calculation between are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94854a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7611, -0.2199], grad_fn=<ViewBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3318,  0.1210, -0.4353],\n",
      "        [ 0.5348,  0.5282, -0.5684]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4340,  0.2342], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([0.1, -0.1, 0.8])\n",
    "\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdfc5",
   "metadata": {},
   "source": [
    "### Sequential layer\n",
    "`Sequential` from `torch.nn` can stack layers such as `Linear` to pass data through the layers in sequence. Layers bookended by the input and output are called hidden layers.\n",
    "\n",
    "A neuron in a linear layer has $n+1$ parameters, with $n$ counting the weight for each input from the previous layer and $1$ accounting the neuron's bias.\n",
    "\n",
    "More hidden layers = more parameters = higher model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e903681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3289d2",
   "metadata": {},
   "source": [
    "Acquire the model's parameters using `parameters()`, which outputs a container of tensors containing each layer's weights and each layer's biases.\n",
    "\n",
    "`numel()` outputs the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2044, -0.3502,  0.4553],\n",
      "        [ 0.5028, -0.4500, -0.4404]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2631,  0.5386], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2358,  0.3472],\n",
      "        [ 0.5581, -0.5005],\n",
      "        [ 0.6865,  0.6474],\n",
      "        [-0.0606, -0.5850],\n",
      "        [ 0.4166, -0.4181],\n",
      "        [-0.6183,  0.1080],\n",
      "        [-0.1166, -0.1088],\n",
      "        [ 0.1808,  0.6320]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6204, -0.4627,  0.0551,  0.1314, -0.2035,  0.5163, -0.3902,  0.7050],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1125,  0.0053,  0.2274,  0.0758, -0.2285, -0.0329,  0.0310, -0.3375],\n",
      "        [-0.0017,  0.0559,  0.3066,  0.2623,  0.0890,  0.0215,  0.1443,  0.1948],\n",
      "        [ 0.3497,  0.0680,  0.0157,  0.3237,  0.1773,  0.3146,  0.0838, -0.3032]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1649,  0.3178, -0.2053], requires_grad=True)\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in sequential_model.parameters():\n",
    "    print(parameter)\n",
    "    count += parameter.numel()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658294cf",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "Type of function that takes a real-valued input (specifically a float) and outputs a single value between 0 and 1. Used for binary classification, and can be placed as the final activation of a network of linear layers after which a forward pass determines classification-or-not by a threshold (for instance 0.5).\n",
    "\n",
    "Equivalent to traditional logistic regression (in that the output is a probability for the category of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0251], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "sigmoid_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28045",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Type of function that takes a one-dimensional input (specifically of floats) and outputs a one-dimensional distribution of probabilities that sum to 1. Used for multi-class classification, and can be placed as the final activation of a network of linear layers after which a forward pass produces the classification to be chosen from the highest per-class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2d56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.6944e-04, 4.9512e-07, 1.6559e-07, 3.1283e-02, 9.6775e-01],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "softmax_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 5),\n",
    "    nn.Softmax()\n",
    ")\n",
    "softmax_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d25b79",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "Greater function that quantifies how far a machine learning model's predictions are from the actual target values, be it during training or in practice. The loss function takes a model prediction $\\hat{y}$ (may be a singular regressive / sigmoid output, or a softmax tensor of probabilities) and ground truth $y$ (the actual value or class itself) as inputs and outputs a single float, the loss.\n",
    "\n",
    "The goal of training is to minimize the loss, which should be low or zero for an accurate prediction and high for an incorrect one.\n",
    "\n",
    "For cross-entropy loss, the ground truth value may be the class itself (a number), so to convert it into a tensor functional against the model prediction (a softmax probability distribution), use `nn.functional.one_hot()` which takes a tensor of indices to make one-hots for and a num_elements and to output a tensor of containing one-hot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dc10bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(0), 3))\n",
    "print(F.one_hot(torch.tensor(1), 3))\n",
    "print(F.one_hot(torch.tensor([0,2]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e4c9a",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss\n",
    "\n",
    "Cross-entropy loss is a common loss function for classification. With a scores tensor (model predictions before the final softmax function) and a one-hot encoded ground truth label as input (both must be converted to floats), the cross-entropy loss function applies an internal softmax to the scores (producing a probability distribution of the same size), then outputs the negative natural log of the ground truth's corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff28c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4235e-05, 9.7807e-01, 2.1880e-02])\n",
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/Documents/GitHub/ml-playground/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.tensor([-5.2, 4.6, 0.8])\n",
    "one_hot_target = F.one_hot(torch.tensor(0), 3)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "print(softmax(scores))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521686e",
   "metadata": {},
   "source": [
    "#### Loss gradients and Backpropagation\n",
    "\n",
    "Loss outputted by a forward pass may vary by the values used for the model's parameters (weights and biases); this rate of change is the loss gradient. With the output (i.e. `loss = criterion(prediction, target)`) of an instantiated loss function (such as `criterion = CrossEntropyLoss()`), we can calculate the gradients of this loss using `loss.backward()`.\n",
    "\n",
    "Backpropagation is the process by which we aim to locate the global minimum of the loss function and tune the model's parameters to better fit. with each forward pass, we incrementally update the model's weights and biases inversely to the loss gradient such that they follow the gradient downstream (gradient descent).\n",
    "\n",
    "The existing gradients of layer `i` can be accessed using `model[i].weight.grad` and `model[i].bias.grad`. To update model parameters manually, access each layer gradient and multiply it by the learning rate, then subtract the result from the weight or bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.1753,  0.1244,  0.2323,  0.1492, -0.1680, -0.2248,  0.0478, -0.0756,\n",
      "         -0.1383, -0.0497,  0.0167, -0.1783,  0.2127, -0.1141, -0.1696,  0.2120],\n",
      "        [ 0.2122,  0.0687,  0.0716,  0.2348, -0.0053, -0.1394, -0.1770,  0.2050,\n",
      "         -0.0140, -0.2197,  0.0857,  0.1603,  0.1718,  0.1180,  0.0837,  0.1293],\n",
      "        [-0.0490,  0.0844, -0.0218, -0.2052, -0.0726,  0.1051,  0.0728,  0.2091,\n",
      "         -0.1908,  0.1536, -0.0643,  0.1927,  0.1669, -0.1224, -0.1305,  0.0416],\n",
      "        [-0.1533, -0.0209, -0.2061,  0.1526,  0.1468,  0.2283, -0.1582, -0.1568,\n",
      "          0.0276,  0.1455,  0.0980,  0.1492,  0.1751, -0.1718, -0.1960, -0.0727],\n",
      "        [ 0.0741,  0.2115,  0.2461,  0.0509,  0.0308,  0.1408,  0.1206, -0.1369,\n",
      "          0.1833, -0.1089,  0.1912, -0.0452,  0.0075,  0.0770,  0.1343, -0.0375],\n",
      "        [-0.1679,  0.1725,  0.1784,  0.1157, -0.0465,  0.1772,  0.1802,  0.1599,\n",
      "         -0.0217, -0.2471,  0.2324,  0.1831, -0.1685, -0.1308, -0.1388, -0.2137],\n",
      "        [ 0.1519, -0.0736,  0.1125,  0.1548,  0.2168,  0.1581, -0.1623, -0.1268,\n",
      "          0.0157, -0.0179,  0.1019, -0.1625,  0.0057, -0.1501, -0.1941, -0.1072],\n",
      "        [-0.1623, -0.0055, -0.0585, -0.2068,  0.0872, -0.0563,  0.1196,  0.0413,\n",
      "          0.1281, -0.1593,  0.1993,  0.2004,  0.1757,  0.1164,  0.2204,  0.1497]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([ 0.0518, -0.2128], requires_grad=True)\n",
      "Gradient of the first layer: None\n",
      "Gradient of the second layer: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGradient of the second layer:\u001b[39m\u001b[33m\"\u001b[39m, grads1)\n\u001b[32m     17\u001b[39m lr = \u001b[32m0.001\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m weight0 -= \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads0\u001b[49m\n\u001b[32m     19\u001b[39m weight1 -= lr * grads1\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Need to compute loss and backpropagate in order to define gradients\n",
    "x = torch.randn(4, 16) # dummy input\n",
    "target = torch.randn(4, 2) # dummy target\n",
    "\n",
    "output = model(x) # dummy output\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "bias1 = model[1].bias\n",
    "print(\"Weight of the first layer:\", weight0)\n",
    "print(\"Bias of the second layer:\", bias1)\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "print(\"Gradient of the first layer:\", grads0)\n",
    "print(\"Gradient of the second layer:\", grads1)\n",
    "\n",
    "lr = 0.001\n",
    "weight0 -= lr * grads0\n",
    "weight1 -= lr * grads1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad353741",
   "metadata": {},
   "source": [
    "#### Optimized gradient descent\n",
    "\n",
    "PyTorch has an optimized gradient descent function, `torch.optim.SGD()`, that uses Stochastic Gradient Descent (which calculates gradients from one or a small subset of training examples, rather than the computationally expensive entire dataset). Takes a model's parameters and a learning rate as input when instantiating; use `optimizer.step()' to perform parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811915f",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Tensors are similar to NumPy arrays but have unique features. Convert a Python list into a Torch tensor with `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7af35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temperatures = [[10, 20], [30, 40], [50, 60]]\n",
    "\n",
    "temperature_tensor = torch.tensor(temperatures)\n",
    "print(temperature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1095cd",
   "metadata": {},
   "source": [
    "Tensors have a `shape` and `dtype`, and can be added elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb4353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums: tensor([[20, 40],\n",
      "        [40, 60],\n",
      "        [60, 80]])\n",
      "Sums shape: torch.Size([3, 2])\n",
      "Sums shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "addend_tensor = torch.tensor([[10, 20], [10, 20], [10, 20]])\n",
    "sums_tensor = temperature_tensor + addend_tensor\n",
    "print(\"Sums:\", sums_tensor)\n",
    "print(\"Sums shape:\", sums_tensor.shape)\n",
    "print(\"Sums shape:\", sums_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797e9f",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "`Linear` from `torch.nn` takes a tensor as input and outputs a tensor whose sizes correspond to `in_features` and `out_features`. The weights and biases involved in the calculation between are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94854a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7825, -0.9039], grad_fn=<ViewBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.1909, -0.4258,  0.5062],\n",
      "        [-0.5135, -0.2065, -0.4223]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3540, -0.5354], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([0.1, -0.1, 0.8])\n",
    "\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdfc5",
   "metadata": {},
   "source": [
    "### Sequential layer\n",
    "`Sequential` from `torch.nn` can stack layers such as `Linear` to pass data through the layers in sequence. Layers bookended by the input and output are called hidden layers.\n",
    "\n",
    "A neuron in a linear layer has $n+1$ parameters, with $n$ counting the weight for each input from the previous layer and $1$ accounting the neuron's bias.\n",
    "\n",
    "More hidden layers = more parameters = higher model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e903681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3289d2",
   "metadata": {},
   "source": [
    "Acquire the model's parameters using `parameters()`, which outputs a container of tensors containing each layer's weights and each layer's biases.\n",
    "\n",
    "`numel()` outputs the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d2321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4372, -0.3712, -0.4777],\n",
      "        [-0.4745, -0.3590,  0.3441]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5434, 0.4214], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6535, -0.6952],\n",
      "        [ 0.3978, -0.6286],\n",
      "        [ 0.0502,  0.2019],\n",
      "        [ 0.2138,  0.1033],\n",
      "        [ 0.2212,  0.0905],\n",
      "        [ 0.4450, -0.1702],\n",
      "        [-0.3050,  0.5096],\n",
      "        [ 0.1094,  0.5195]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2077,  0.0894, -0.6579,  0.4189,  0.4407,  0.0286, -0.2390,  0.1748],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0654,  0.0603,  0.1492,  0.0654,  0.0913, -0.1673,  0.2176,  0.1341],\n",
      "        [ 0.2890, -0.1768, -0.0975,  0.0136,  0.2267, -0.1219,  0.0688, -0.2123],\n",
      "        [ 0.2221, -0.2042, -0.2308,  0.1172,  0.0818, -0.2777,  0.2426,  0.1920]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1055,  0.3396, -0.1689], requires_grad=True)\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in sequential_model.parameters():\n",
    "    print(parameter)\n",
    "    count += parameter.numel()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658294cf",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "Type of function that takes a real-valued input (specifically a float) and outputs a single value between 0 and 1. Used for binary classification, and can be placed as the final activation of a network of linear layers after which a forward pass determines classification-or-not by a threshold (for instance 0.5).\n",
    "\n",
    "Equivalent to traditional logistic regression (in that the output is a probability for the category of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34d2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5200], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "sigmoid_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28045",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Type of function that takes a one-dimensional input (specifically of floats) and outputs a one-dimensional distribution of probabilities that sum to 1. Used for multi-class classification, and can be placed as the final activation of a network of linear layers after which a forward pass produces the classification to be chosen from the highest per-class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04f2d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/Documents/GitHub/ml-playground/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8374, 0.0024, 0.0745, 0.0015, 0.0842], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "softmax_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 5),\n",
    "    nn.Softmax()\n",
    ")\n",
    "softmax_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d25b79",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "The function that quantifies how far a machine learning model's predictions are from the actual target values, be it during training or in practice. The loss function takes a model prediction $\\hat{y}$ (may be a singular regressive / sigmoid output, or a softmax tensor of probabilities) and ground truth $y$ (the actual value or class itself) as inputs and outputs a single float, the loss.\n",
    "\n",
    "The goal of training is to minimize the loss, which should be low or zero for an accurate prediction and high for an incorrect one.\n",
    "\n",
    "For cross-entropy loss, the ground truth value may be the class itself (a number), so to convert it into a tensor functional against the model prediction (a softmax probability distribution), use `nn.functional.one_hot()` which takes a tensor of indices to make one-hots for and a num_elements and to output a tensor of containing one-hot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dc10bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(0), 3))\n",
    "print(F.one_hot(torch.tensor(1), 3))\n",
    "print(F.one_hot(torch.tensor([0,2]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e4c9a",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss\n",
    "\n",
    "Cross-entropy loss is a common loss function for classification. With a scores tensor (model predictions before the final softmax function) and a one-hot encoded ground truth label as input (both must be converted to floats), the cross-entropy loss function applies an internal softmax to the scores (producing a probability distribution of the same size), then outputs the negative natural log of the ground truth's corresponding probability.\n",
    "\n",
    "Cross-entropy loss function is supplied in PyTorch by instantiating `nn.CrossEntropyLoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff28c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4235e-05, 9.7807e-01, 2.1880e-02])\n",
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.tensor([-5.2, 4.6, 0.8])\n",
    "one_hot_target = F.one_hot(torch.tensor(0), 3)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "print(softmax(scores))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521686e",
   "metadata": {},
   "source": [
    "#### Loss gradients and Backpropagation\n",
    "\n",
    "Loss outputted by a forward pass may vary by the values used for the model's parameters (weights and biases); this rate of change is the loss gradient. With the output (i.e. `loss = criterion(prediction, target)`) of an instantiated loss function (such as `criterion = CrossEntropyLoss()`), we can calculate the gradients of this loss using `loss.backward()`.\n",
    "\n",
    "Backpropagation is the process by which we aim to locate the global minimum of the loss function and tune the model's parameters to better fit. with each forward pass, we incrementally update the model's weights and biases inversely to the loss gradient such that they follow the gradient downstream (gradient descent).\n",
    "\n",
    "The existing gradients of layer `i` can be accessed using `model[i].weight.grad` and `model[i].bias.grad`. To update model parameters manually, access each layer gradient and multiply it by the learning rate, then subtract the result from the weight or bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45d191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.3206, -0.3639,  0.3761, -0.4230, -0.1317],\n",
      "        [ 0.3998, -0.2565,  0.4259,  0.0580, -0.2682],\n",
      "        [ 0.1348,  0.2798,  0.2490,  0.1747, -0.3247],\n",
      "        [-0.2314, -0.3041,  0.4000,  0.3989, -0.1950],\n",
      "        [-0.2652,  0.2406,  0.3434, -0.1151,  0.1719],\n",
      "        [-0.0882,  0.0454, -0.3510,  0.0415, -0.0767],\n",
      "        [ 0.2033,  0.3179, -0.2007,  0.0304, -0.1262],\n",
      "        [-0.2840, -0.2974, -0.2339,  0.1518,  0.0749]], requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([ 0.1527, -0.3415], requires_grad=True)\n",
      "Gradient of the first layer: tensor([[ 0.0045,  0.0348, -0.0262,  0.0470,  0.0377],\n",
      "        [ 0.0942,  0.7219, -0.5435,  0.9740,  0.7812],\n",
      "        [ 0.0820,  0.6286, -0.4732,  0.8481,  0.6802],\n",
      "        [ 0.0729,  0.5587, -0.4206,  0.7538,  0.6045],\n",
      "        [-0.0388, -0.2971,  0.2237, -0.4008, -0.3215],\n",
      "        [ 0.0782,  0.5995, -0.4513,  0.8088,  0.6487],\n",
      "        [-0.0241, -0.1848,  0.1391, -0.2494, -0.2000],\n",
      "        [ 0.0076,  0.0580, -0.0437,  0.0782,  0.0628]])\n",
      "Gradient of the second layer: tensor([[-1.5181, -1.4358,  0.7204, -1.0607, -0.6195,  1.1818,  1.0049, -0.2656],\n",
      "        [ 1.5181,  1.4358, -0.7204,  1.0607,  0.6195, -1.1818, -1.0049,  0.2656]])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 8),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Need to compute loss and backpropagate in order to define gradients\n",
    "\n",
    "x = torch.randn(5) # dummy input\n",
    "target = torch.randn(2) # dummy target\n",
    "output = model(x) # dummy prediction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "bias1 = model[1].bias\n",
    "print(\"Weight of the first layer:\", weight0)\n",
    "print(\"Bias of the second layer:\", bias1)\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "print(\"Gradient of the first layer:\", grads0)\n",
    "print(\"Gradient of the second layer:\", grads1)\n",
    "\n",
    "lr = 0.001\n",
    "weight0 = weight0 - lr * grads0 # Note that -= does not work, as grad requires being used in an in-place operation\n",
    "weight1 = weight1 - lr * grads1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad353741",
   "metadata": {},
   "source": [
    "#### Optimized gradient descent\n",
    "\n",
    "PyTorch has an optimized gradient descent function, `torch.optim.SGD()`, that uses Stochastic Gradient Descent (which calculates gradients from one or a small subset of training examples, rather than the computationally expensive entire dataset). Takes a model's parameters and a learning rate as input when instantiating; use `optimizer.step()` to perform an update on all of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "251132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "x = torch.randn(5) # dummy input\n",
    "target = torch.randn(2) # dummy target\n",
    "output = model(x) # dummy prediction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f629c",
   "metadata": {},
   "source": [
    "#### DataFrames\n",
    "\n",
    "`pd.read_csv()` from `pandas` reads into a `pandas.DataFrame` from a .csv file.\n",
    "\n",
    "The `DataFrame.iloc` property is used to select slices of the DataFrame through integer-location based indexing. `iloc[:,1:-1]` for instance selects all but the first and last columns (as columns are dimension 1 of the DataFrame).\n",
    "\n",
    "`to_numpy()` of a DataFrame outputs an equivalent numpy array for easier handling with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample:  tensor([0., 1.], dtype=torch.float64)\n",
      "Label sample:  tensor([2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = pd.read_csv(\"dataset.csv\")\n",
    "# target = dataset.iloc[:, -1]\n",
    "# y = target.to_numpy()\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f2fca",
   "metadata": {},
   "source": [
    "#### Data loading and batching\n",
    "\n",
    "`TensorDataset` and `DataLoader` from `torch.utils.data` help manage data loading and batching during training. \n",
    "\n",
    "Instantiate a `DataLoader` with the following parameters:\n",
    "- the `TensorDataset` itself\n",
    "- `batch_size` determines the number of samples included in each iteration\n",
    "- `shuffle` is whether the data order at each epoch (a full pass through the training data) is randomized, which helps improve model generalization (level of performance on unseen data).\n",
    "\n",
    "The `DataLoader` behaves like a PEZ dispenser; each element in it is a tuple unpacked as batch_inputs (features) and batch_labels (target), representing a row from the dataset. Iterating through the `DataLoader` dispenses feature and label tensors that are `batch_size` in depth (aka `batch_size` number of samples, or fewer if there are no more left), corresponding to `batch_size` rows in the `TensorDataset`. \n",
    "\n",
    "In real-world deep learning, batch sizes are often 32 or greater to accommodate larger datasets with better computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9668f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: tensor([0., 1.], dtype=torch.float64)\n",
      "Label sample: tensor([2.], dtype=torch.float64)\n",
      "Batch inputs: tensor([[ 0.,  1.],\n",
      "        [-1.,  0.]], dtype=torch.float64)\n",
      "Batch labels: tensor([[2.],\n",
      "        [4.]], dtype=torch.float64)\n",
      "Batch inputs: tensor([[-1., -1.],\n",
      "        [-1.,  1.]], dtype=torch.float64)\n",
      "Batch labels: tensor([[1.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "Batch inputs: tensor([[1., 0.]], dtype=torch.float64)\n",
      "Batch labels: tensor([[3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X = np.array([[0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [-1.0, 0.0],\n",
    "              [-1.0, 1.0],\n",
    "              [-1.0, -1.0]])\n",
    "y = np.array([[2.0],\n",
    "              [3.0],\n",
    "              [4.0],\n",
    "              [0.0],\n",
    "              [1.0]])\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "\n",
    "input_sample, label_sample = dataset[0]\n",
    "print(\"Input sample:\", input_sample)\n",
    "print(\"Label sample:\", label_sample)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print(\"Batch inputs:\", batch_inputs)\n",
    "    print(\"Batch labels:\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2165c",
   "metadata": {},
   "source": [
    "#### Mean Squared Error loss\n",
    "\n",
    "Mean Squared Error (MSE) loss is a common loss function for regression. With a predictions tensor and ground truth tensor as input (both must be converted to floats), MSE loss is the average of the squared difference between each prediction and its corresponding ground truth.\n",
    "\n",
    "The MSE loss function is implemented in NumPy as the function below, and is supplied in PyTorch by instantiating `nn.MSELoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea8a2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "    return np.mean((prediction - target) ** 2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# loss = criterion(torch.tensor(prediction), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebee68e",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "Training a neural network involves creating a model, choosing a loss function, defining a dataset, setting an optimizer, and running the training loop (calculating loss via a forward pass, computing gradients via backpropagation, and updating model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Loop over the number of epochs and then the dataloader\n",
    "for i in num_epochs:\n",
    "  for data in ____:\n",
    "    # Set the gradients to zero\n",
    "    ____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

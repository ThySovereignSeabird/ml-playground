{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811915f",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Tensors are similar to NumPy arrays but have unique features. Convert a Python list into a Torch tensor with `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7af35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temperatures = [[10, 20], [30, 40], [50, 60]]\n",
    "\n",
    "temperature_tensor = torch.tensor(temperatures)\n",
    "print(temperature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1095cd",
   "metadata": {},
   "source": [
    "Tensors have a `shape` and `dtype`, and can be added elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb4353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums: tensor([[20, 40],\n",
      "        [40, 60],\n",
      "        [60, 80]])\n",
      "Sums shape: torch.Size([3, 2])\n",
      "Sums shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "addend_tensor = torch.tensor([[10, 20], [10, 20], [10, 20]])\n",
    "sums_tensor = temperature_tensor + addend_tensor\n",
    "print(\"Sums:\", sums_tensor)\n",
    "print(\"Sums shape:\", sums_tensor.shape)\n",
    "print(\"Sums shape:\", sums_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797e9f",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "`Linear` from `torch.nn` takes a tensor as input and outputs a tensor whose sizes correspond to `in_features` and `out_features`. The weights and biases involved in the calculation between are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94854a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4454, -0.5690], grad_fn=<ViewBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.3867, -0.0119, -0.0820],\n",
      "        [ 0.3435, -0.5435, -0.3046]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5484, -0.4140], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([0.1, -0.1, 0.8])\n",
    "\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdfc5",
   "metadata": {},
   "source": [
    "### Sequential layer\n",
    "`Sequential` from `torch.nn` can stack layers such as `Linear` to pass data through the layers in sequence. Layers bookended by the input and output are called hidden layers.\n",
    "\n",
    "A neuron in a linear layer has $n+1$ parameters, with $n$ counting the weight for each input from the previous layer and $1$ accounting the neuron's bias.\n",
    "\n",
    "More hidden layers = more parameters = higher model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e903681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3289d2",
   "metadata": {},
   "source": [
    "Acquire the model's parameters using `parameters()`, which outputs a container of tensors containing each layer's weights and each layer's biases.\n",
    "\n",
    "`numel()` outputs the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d2321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0043, -0.3726, -0.2042],\n",
      "        [ 0.1532,  0.0616, -0.4301]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2986, -0.3719], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0132,  0.3602],\n",
      "        [ 0.1083,  0.2950],\n",
      "        [-0.3910,  0.2208],\n",
      "        [-0.5053,  0.1646],\n",
      "        [ 0.5056,  0.3109],\n",
      "        [ 0.4946, -0.4006],\n",
      "        [ 0.0100, -0.0050],\n",
      "        [ 0.6591, -0.0147]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2311, -0.0944,  0.3947, -0.0409,  0.6380,  0.4827,  0.2862,  0.4078],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3282,  0.1979, -0.1349,  0.1230, -0.0341,  0.2138, -0.1414,  0.0091],\n",
      "        [-0.3316, -0.1466,  0.0412,  0.3316,  0.2403, -0.0737, -0.2166,  0.2909],\n",
      "        [-0.1337, -0.0096,  0.2199,  0.2130,  0.0700, -0.1754,  0.1948, -0.2480]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1423, 0.0299, 0.2928], requires_grad=True)\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in sequential_model.parameters():\n",
    "    print(parameter)\n",
    "    count += parameter.numel()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658294cf",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "Type of function that takes a real-valued input (specifically a float) and outputs a single value between 0 and 1. Used for binary classification, and can be placed as the final activation of a network of linear layers after which a forward pass determines classification-or-not by a threshold (for instance 0.5).\n",
    "\n",
    "Equivalent to traditional logistic regression (in that the output is a probability for the category of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34d2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8874], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "sigmoid_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28045",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Type of function that takes a one-dimensional input (specifically of floats) and outputs a one-dimensional distribution of probabilities that sum to 1. Used for multi-class classification, and can be placed as the final activation of a network of linear layers after which a forward pass produces the classification to be chosen from the highest per-class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04f2d56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0023e-01, 1.6290e-02, 4.6548e-05, 8.8521e-04, 6.8255e-01],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "softmax_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 5),\n",
    "    nn.Softmax()\n",
    ")\n",
    "softmax_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d25b79",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "The function that quantifies how far a machine learning model's predictions are from the actual target values, be it during training or in practice. The loss function takes a model prediction $\\hat{y}$ (may be a singular regressive / sigmoid output, or a softmax tensor of probabilities) and ground truth $y$ (the actual value or class itself) as inputs and outputs a single float, the loss.\n",
    "\n",
    "The goal of training is to minimize the loss, which should be low or zero for an accurate prediction and high for an incorrect one.\n",
    "\n",
    "For cross-entropy loss, the ground truth value may be the class itself (a number), so to convert it into a tensor functional against the model prediction (a softmax probability distribution), use `nn.functional.one_hot()` which takes a tensor of indices to make one-hots for and a num_elements and to output a tensor of containing one-hot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dc10bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(0), 3))\n",
    "print(F.one_hot(torch.tensor(1), 3))\n",
    "print(F.one_hot(torch.tensor([0,2]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e4c9a",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss\n",
    "\n",
    "Cross-entropy loss is a common loss function for classification. With a scores tensor (model predictions before the final softmax function) and a one-hot encoded ground truth label as input (both must be converted to floats), the cross-entropy loss function applies an internal softmax to the scores (producing a probability distribution of the same size), then outputs the negative natural log of the ground truth's corresponding probability.\n",
    "\n",
    "Cross-entropy loss function is supplied in PyTorch by instantiating `nn.CrossEntropyLoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff28c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4235e-05, 9.7807e-01, 2.1880e-02])\n",
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.tensor([-5.2, 4.6, 0.8])\n",
    "one_hot_target = F.one_hot(torch.tensor(0), 3)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "print(softmax(scores))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521686e",
   "metadata": {},
   "source": [
    "#### Loss gradients and Backpropagation\n",
    "\n",
    "Loss outputted by a forward pass may vary by the values used for the model's parameters (weights and biases); this rate of change is the loss gradient. With the output (i.e. `loss = criterion(prediction, target)`) of an instantiated loss function (such as `criterion = CrossEntropyLoss()`), we can calculate the gradients of this loss using `loss.backward()`.\n",
    "\n",
    "Backpropagation is the process by which we aim to locate the global minimum of the loss function and tune the model's parameters to better fit. with each forward pass, we incrementally update the model's weights and biases inversely to the loss gradient such that they follow the gradient downstream (gradient descent).\n",
    "\n",
    "The existing gradients of layer `i` can be accessed using `model[i].weight.grad` and `model[i].bias.grad`. To update model parameters manually, access each layer gradient and multiply it by the learning rate, then subtract the result from the weight or bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45d191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[-0.3165,  0.1104, -0.1071,  0.4049, -0.3143],\n",
      "        [-0.0234, -0.3527, -0.0361,  0.0084, -0.4175],\n",
      "        [-0.1854,  0.3420, -0.3655,  0.2279, -0.1146],\n",
      "        [ 0.1683,  0.1802,  0.1531,  0.2364,  0.0681],\n",
      "        [-0.3879, -0.1752, -0.2330,  0.2276, -0.3617],\n",
      "        [-0.2367, -0.4007, -0.3831,  0.2490,  0.2312],\n",
      "        [-0.1730, -0.0632, -0.3578,  0.3751,  0.2336],\n",
      "        [-0.4034, -0.0928,  0.2454, -0.3865,  0.3348]], requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([-0.3145, -0.1655], requires_grad=True)\n",
      "Gradient of the first layer: tensor([[-0.0101, -0.0120,  0.0985, -0.0688,  0.0205],\n",
      "        [ 0.0016,  0.0020, -0.0161,  0.0112, -0.0033],\n",
      "        [ 0.0040,  0.0048, -0.0396,  0.0277, -0.0083],\n",
      "        [-0.0045, -0.0054,  0.0443, -0.0310,  0.0092],\n",
      "        [ 0.0115,  0.0137, -0.1123,  0.0785, -0.0234],\n",
      "        [-0.0082, -0.0098,  0.0804, -0.0562,  0.0167],\n",
      "        [-0.0119, -0.0143,  0.1167, -0.0816,  0.0243],\n",
      "        [-0.0047, -0.0057,  0.0464, -0.0324,  0.0097]])\n",
      "Gradient of the second layer: tensor([[ 0.1863,  0.0858,  0.2104,  0.0235,  0.0526,  0.0843,  0.0974, -0.1480],\n",
      "        [-0.1863, -0.0858, -0.2104, -0.0235, -0.0526, -0.0843, -0.0974,  0.1480]])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 8),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Need to compute loss and backpropagate in order to define gradients\n",
    "\n",
    "x = torch.randn(5) # dummy input\n",
    "target = torch.randn(2) # dummy target\n",
    "output = model(x) # dummy prediction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "bias1 = model[1].bias\n",
    "print(\"Weight of the first layer:\", weight0)\n",
    "print(\"Bias of the second layer:\", bias1)\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "print(\"Gradient of the first layer:\", grads0)\n",
    "print(\"Gradient of the second layer:\", grads1)\n",
    "\n",
    "lr = 0.001\n",
    "weight0 = weight0 - lr * grads0 # Note that -= does not work, as grad requires being used in an in-place operation\n",
    "weight1 = weight1 - lr * grads1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad353741",
   "metadata": {},
   "source": [
    "#### Optimized gradient descent\n",
    "\n",
    "PyTorch has an optimized gradient descent function, `torch.optim.SGD()`, that uses Stochastic Gradient Descent (which calculates gradients from one or a small subset of training examples, rather than the computationally expensive entire dataset). Takes a model's parameters and a learning rate as input when instantiating; use `optimizer.step()` to perform an update on all of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "251132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "x = torch.randn(5) # dummy input\n",
    "target = torch.randn(2) # dummy target\n",
    "output = model(x) # dummy prediction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f629c",
   "metadata": {},
   "source": [
    "#### DataFrames\n",
    "\n",
    "`pd.read_csv()` from `pandas` reads into a `pandas.DataFrame` from a .csv file.\n",
    "\n",
    "The `DataFrame.iloc` property is used to select slices of the DataFrame through integer-location based indexing. `iloc[:,1:-1]` for instance selects all but the first and last columns (as columns are dimension 1 of the DataFrame).\n",
    "\n",
    "`to_numpy()` of a DataFrame outputs an equivalent numpy array for easier handling with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a0f0dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = pd.read_csv(\"dataset.csv\")\n",
    "# target = dataset.iloc[:, -1]\n",
    "# y = target.to_numpy()\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f2fca",
   "metadata": {},
   "source": [
    "#### Data loading and batching\n",
    "\n",
    "`TensorDataset` and `DataLoader` from `torch.utils.data` help manage data loading and batching during training. \n",
    "\n",
    "Instantiate a `DataLoader` with the following parameters:\n",
    "- the `TensorDataset` itself\n",
    "- `batch_size` determines the number of samples included in each iteration\n",
    "- `shuffle` is whether the data order at each epoch (a full pass through the training data) is randomized, which helps improve model generalization (level of performance on unseen data).\n",
    "\n",
    "The `DataLoader` behaves like a PEZ dispenser; each element in it is a tuple unpacked as batch_inputs (features) and batch_labels (target), representing a row from the dataset. Iterating through the `DataLoader` dispenses feature and label tensors that are `batch_size` in depth (aka `batch_size` number of samples, or fewer if there are no more left), corresponding to `batch_size` rows in the `TensorDataset`. Resets every time it is used as an iterator.\n",
    "\n",
    "In real-world deep learning, batch sizes are often 32 or greater to accommodate larger datasets with better computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9668f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: tensor([0., 1.])\n",
      "Label sample: tensor([2.])\n",
      "Batch inputs: tensor([[-1.,  0.],\n",
      "        [ 0.,  1.]])\n",
      "Batch labels: tensor([[4.],\n",
      "        [2.]])\n",
      "Batch inputs: tensor([[-1., -1.],\n",
      "        [-1.,  1.]])\n",
      "Batch labels: tensor([[1.],\n",
      "        [0.]])\n",
      "Batch inputs: tensor([[1., 0.]])\n",
      "Batch labels: tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X = np.array([[0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [-1.0, 0.0],\n",
    "              [-1.0, 1.0],\n",
    "              [-1.0, -1.0]], dtype=np.float32)\n",
    "y = np.array([[2.0],\n",
    "              [3.0],\n",
    "              [4.0],\n",
    "              [0.0],\n",
    "              [1.0]], dtype=np.float32)\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "\n",
    "input_sample, label_sample = dataset[0]\n",
    "print(\"Input sample:\", input_sample)\n",
    "print(\"Label sample:\", label_sample)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print(\"Batch inputs:\", batch_inputs)\n",
    "    print(\"Batch labels:\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2165c",
   "metadata": {},
   "source": [
    "#### Mean Squared Error loss\n",
    "\n",
    "Mean Squared Error (MSE) loss is a common loss function for regression. With a predictions tensor and ground truth tensor as input (both must be converted to floats), MSE loss is the average of the squared difference between each prediction and its corresponding ground truth.\n",
    "\n",
    "The MSE loss function is implemented in NumPy as the function below, and is supplied in PyTorch by instantiating `nn.MSELoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ea8a2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "    return np.mean((prediction - target) ** 2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# loss = criterion(torch.tensor(prediction), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebee68e",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "Training a neural network involves:\n",
    "- creating a model\n",
    "- choosing a loss function\n",
    "- defining a dataset\n",
    "- setting an optimizer (optimized gradient descent function)\n",
    "- and running the training loop (calculating loss via a forward pass, computing gradients via backpropagation, and updating model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e01f3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop over the number of epochs and then the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader: # data is a tuple with features and target\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get feature and target from the dataloader\n",
    "    feature, target = data\n",
    "\n",
    "    # Run a forward pass\n",
    "    prediction = model(feature)\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    loss = criterion(prediction, target)\n",
    "    loss.backward() # Update gradients\n",
    "\n",
    "    # Descend gradients / update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962a112",
   "metadata": {},
   "source": [
    "#### ReLU activation functions\n",
    "\n",
    "Sometimes activation functions can shrink gradients too much, reducing training efficiency:\n",
    "- Sigmoid and softmax functions bound their outputs between 0 and 1, so have very small gradients for large and small values of $x$ (namely, as the sigmoid output approaches either 0 or 1).\n",
    "- This phenomenon is called saturation, which introduces the vanishing gradients problem (each gradient depends on the previous one during backpropagation, so extremely small gradients can fail to update the weights effectively). This is also what renders sigmoid and softmax suboptimal for hidden layers (use them in the last layer only).\n",
    "\n",
    "Common activation functions designed for use in hidden layers include:\n",
    "- The Rectified Linear Unit (ReLU), defined by the function $f(x) = max(x, 0)$, converts all negative inputs to 0. Its gradients do not approach 0 for large values of $x$, circumventing the vanishing gradients problem. \n",
    "    - ReLU is supplied in PyTorch by instantiating `nn.ReLU()`.\n",
    "- Leaky ReLU is a variation that scales negative inputs by a very small coefficient (default 0.01 in PyTorch), preventing neurons from ceasing learning with zero-value gradients which can occur with standard ReLU.\n",
    "    - Leaky ReLU is supplied in PyTorch by instantiating `nn.LeakyReLU()` with a `negative_slope` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf319120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU applied to positive x: tensor(2.)\n",
      "ReLU applied to negative x: tensor(0.)\n",
      "ReLU applied to positive x: tensor(2.)\n",
      "ReLU applied to negative x: tensor(-0.1500)\n"
     ]
    }
   ],
   "source": [
    "x_pos = torch.tensor(2.0)\n",
    "x_neg = torch.tensor(-3.0)\n",
    "\n",
    "relu_torch = nn.ReLU()\n",
    "print(\"ReLU applied to positive x:\", relu_torch(x_pos))\n",
    "print(\"ReLU applied to negative x:\", relu_torch(x_neg))\n",
    "\n",
    "leakyrelu_torch = nn.LeakyReLU(0.05)\n",
    "print(\"ReLU applied to positive x:\", leakyrelu_torch(x_pos))\n",
    "print(\"ReLU applied to negative x:\", leakyrelu_torch(x_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475888f",
   "metadata": {},
   "source": [
    "#### Learning rate and momentum\n",
    "\n",
    "Recall that training a neural network is optimizing its parameters to achieve minimum loss, and the Stochastic Gradient Descent (SGD) optimizer is what descends the model's gradients to this end.\n",
    "\n",
    "`optim.SGD()` actually takes two key arguments after the model's parameters:\n",
    "- `lr` (the learning rate) controls the step size; larger increases efficiency, smaller increases precision and avoids getting stuck going back-and-forth about minima.\n",
    "    - Typical range: `0.01` to `0.0001`. Rule of thumb: start with `0.001`.\n",
    "- `momentum` adds inertia to help the optimizer avoid getting stuck in local minima; with larger momentum, steep drops in loss push the gradient descent further.\n",
    "    - Typical range: `0.85` to `0.99`. Rule of thumb: start with `0.95`.\n",
    "\n",
    "Momentum is crucial when the global minimum to be found is of a non-convex (i.e. local minima-riddled) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "306f867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e8041",
   "metadata": {},
   "source": [
    "#### Layer initialization\n",
    "\n",
    "The weights of a layer are often initialized to small values; PyTorch's `nn.Linear` for instance initializes its weights between roughly `-0.125` and `0.125`. Keeping both the input data and layer weights small ensures stable training and prevents extreme values that could slow training.\n",
    "\n",
    "`nn.init.uniform_()` takes a tensor (like the `weight` property of a layer such as `nn.Linear`) as input and initializes it with a uniform distribution between 0 and 1.\n",
    "\n",
    "#### Transfer learning\n",
    "\n",
    "Transfer learning is the reuse of a model trained on a first task for a second similar task. For instance, a model trained on US data scientist salaries can be repurposed as a model to train on European salaries, leveraging the latent information within the original model's weights as a starting point.\n",
    "\n",
    "Save and load weights to and from local files using `torch.save(layer, \"layer.pth\")` and `torch.load(\"layer.pth\")`, which work on any type of PyTorch objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ed10f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6822e-06, grad_fn=<MinBackward1>) tensor(0.9999, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(64, 128)\n",
    "\n",
    "# torch.save(layer, \"layer.pth\")\n",
    "# new_layer = torch.load(\"layer.pth\")\n",
    "nn.init.uniform_(layer.weight)\n",
    "print(layer.weight.min(), layer.weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54c674",
   "metadata": {},
   "source": [
    "#### Fine-tuning\n",
    "\n",
    "A type of transfer learning where we load weights from a previously trained model and fine-tune them with a small learning rate and/or with freeze certain parts of the network (often the early ones, whereas the layers closer to the output get fine-tuned).\n",
    "\n",
    "Fine-tuning process:\n",
    "- Find a model trained on a similar task\n",
    "- Load pre-trained weights\n",
    "- Optionally freeze some of the layers in the model\n",
    "- Train with a smaller learning rate\n",
    "- Inspect loss for if learning rate needs to be adjusted\n",
    "\n",
    "Fine-tuning existing models is useful enough that deep learning engineers oftentimes rarely train their model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b628060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(64, 128),\n",
    "    nn.Linear(128, 64)\n",
    ")\n",
    "\n",
    "# named_parameters returns the name and parameter itself\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"0_weight\":\n",
    "        param.requires_grad = False\n",
    "        # setting this to False ceases gradient tracking,\n",
    "        # which essentially marks these as ignored\n",
    "        # by the optmizer thereby preventing further\n",
    "        # updates to the layer's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c00e0",
   "metadata": {},
   "source": [
    "#### Evaluating model performance\n",
    "\n",
    "A dataset is typically split into three subsets:\n",
    "- training (80-90%) to adjust model parameters\n",
    "- validation (10-20%) to tune hyperparameters such as k in k-nearest, learning rate, and momentum\n",
    "- test (5-10%) to evaluate final model performance on unseen data\n",
    "\n",
    "Key metrics to track include loss and accuracy during training and validation. For each epoch:\n",
    "- Training/validation loss is the sum of losses across all batches in the dataloader; can compute the mean training/validation loss by dividing total loss by the number of batches\n",
    "\n",
    "When a model overfits, training loss keeps decreasing but validation loss starts to rise, which reflects a loss of generality in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c834cae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "  (1): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential( # dummy model\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "X = np.array([[0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [-1.0, 0.0],\n",
    "              [-1.0, 1.0],\n",
    "              [-1.0, -1.0]], dtype=np.float32)\n",
    "y = np.array([[2.0],\n",
    "              [3.0],\n",
    "              [4.0],\n",
    "              [0.0],\n",
    "              [1.0]], dtype=np.float32)\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# @@@ Calculating training loss @@@\n",
    "\n",
    "training_loss = 0\n",
    "for inputs, labels in dataloader: # containing training data\n",
    "    # Run the forward pass\n",
    "    outputs = model(inputs)\n",
    "    # Compute loss from prediction and ground truth\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Backpropagation:\n",
    "    loss.backward # Computer gradients\n",
    "    optimizer.step() # Update weights\n",
    "    optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "    # Calc and sum the loss\n",
    "    training_loss += loss.item()\n",
    "epoch_loss = training_loss / len(dataloader) # mean loss\n",
    "\n",
    "# @@@ Calculating validation loss @@@\n",
    "\n",
    "validation_loss = 0\n",
    "model.eval() # put model in evaluation mode,\n",
    "# as some layers behave differently during training and validation\n",
    "\n",
    "with torch.no_grad(): # disable gradients for efficiency\n",
    "    for inputs, labels in dataloader: # containing validation data\n",
    "        # Run the forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute loss from prediction and ground truth\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calc and sum the loss\n",
    "        validation_loss += loss.item() # .item() turns the (1,) tensor into a numerical value\n",
    "epoch_loss = validation_loss / len(dataloader) # mean loss\n",
    "model.train() # switch back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad62c1",
   "metadata": {},
   "source": [
    "#### Accuracy with torchmetrics\n",
    "\n",
    "Loss describes how well a model is learning, but not necessarily how accurately it makes predictions.\n",
    "\n",
    "For multi-class classification tasks, create an accuracy metric with `torchmetrics.Accuracy`. As the model processes each batch, the metric updates by taking predictions and ground truth labels as input (similarly to a loss function).\n",
    "\n",
    "The model outputs a probability distribution over multiple classes, so use `argmax(dim=-1)` to select the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66859272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "# Create accuracy metric\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "for features, labels in dataloader:\n",
    "    outputs = model(features) # Do a forward pass\n",
    "    # Compute batch accuracy (selecting argmax for one-hot labels)\n",
    "    # metric.update(outputs, labels.argmax(dim=-1))\n",
    "\n",
    "# Compute accuracy over entire epoch\n",
    "accuracy = metric.compute()\n",
    "\n",
    "# Reset metric for next epoch\n",
    "metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f1352",
   "metadata": {},
   "source": [
    "#### Fighting overfitting \n",
    "\n",
    "Overfitting is when the model falters in generalizing to unseen data (performs well on training data but poorly on validation data). Possible causes:\n",
    "- If the dataset is not large enough, solutions are to get more data or synthesize more through data augmentation\n",
    "- If the model has too much capacity, reduce the model size or add a \"dropout\" layer\n",
    "- If the weights are too large, use weight decay to force parameters to remain small\n",
    "\n",
    "#### Dropout layer\n",
    "\n",
    "\"Regularization\" through a dropout layer is a technique that randomly zeroes out elements of the input tensor during training, preventing the model from becoming too dependent on specific features from the training dataset. Dropout layers are typically added after activation functions. Dropouts are added after the activation function and behaves differently during `model.train()` vs. `model.eval()` (specifically, its zeroing is intended to be disabled during validation).\n",
    "\n",
    "This layer is supplied in PyTorch by `nn.Dropout()`, where the `p` argument is the probability that a neuron (element in the input tensor) is zeroed.\n",
    "\n",
    "#### Weight decay\n",
    "\n",
    "Weight decay is another form of regularization, supplied in PyTorch as an optimizer parameter (`weight_decay`) typically set to a small value like `0.0001`. This parameter adds a penalty to the loss function, encouraging smaller weights and improving generalization. During backpropagation this penalty is subtracted from the gradient to prevent excessive weight growth, and higher weight decay entails stronger regularization against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02e215ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8662, 0.0000, 0.0000, 0.7566]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential( # dummy model\n",
    "    nn.Linear(8, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5)\n",
    ")\n",
    "features = torch.randn((1, 8))\n",
    "print(model(features))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5c2a1",
   "metadata": {},
   "source": [
    "#### Maximizing model performance\n",
    "\n",
    "Steps to maximize performance:\n",
    "\n",
    "#### 1. Create a model that overfits the training set (be it a single data point or a small subset)\n",
    "- This ensures that the problem is solvable and catch potential roadblocks early on.\n",
    "- Rig the training loop to repeatedly train on a single example rather than the entire dataloader; a proper model and an applicable problem should quickly descend to near-zero loss and 100% accuracy on that data point.\n",
    "- Then, scale up to the entire training set; at this stage we use an existing model architecture large enough to overfit while keeping hyperparameters (e.g. learning rate, momentum) at their defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac38bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "features, labels = next(iter(dataloader))\n",
    "for i in range(1000):\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdab595",
   "metadata": {},
   "source": [
    "#### 2. Maximize the validation accuracy\n",
    "- Set a performance baseline to aim for with the validation set, then use strategies to reduce overfitting and help the model generalize well to unseen data: dropout, data augmentation, weight decay, or downsizing model capacity.\n",
    "- Regularization comes with its costs: too little can fail to tackle overfitting and helping the model generalize to unseen data, while too much can reduce training / validation accuracy and limit the model's ability to learn effectively.\n",
    "\n",
    "#### 3. Fine-tune hyperparameters\n",
    "- Often done on optimizer settings such as learning rate and momentum.\n",
    "- Methods of testing different hyperparameter values:\n",
    "    - Grid search tests parameters at fixed intervals\n",
    "    - Random search randomly selects values within a given range, and is often more efficient as it avoids unnecessary tests and increases the chance of finding optimal settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c19049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search from 10^-2 to 10^-6\n",
    "for factor in range(2,6):\n",
    "    lr = 10 ** -factor\n",
    "\n",
    "# Random search from 10^-2 to 10^-6\n",
    "factor = np.random.uniform(2, 6) # a single number\n",
    "lr = 10 ** -factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

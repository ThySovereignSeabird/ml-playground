{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811915f",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch\n",
    "Tensors are similar to NumPy arrays but have unique features. Convert a Python list into a Torch tensor with `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7af35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temperatures = [[10, 20], [30, 40], [50, 60]]\n",
    "\n",
    "temperature_tensor = torch.tensor(temperatures)\n",
    "print(temperature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1095cd",
   "metadata": {},
   "source": [
    "Tensors have a `shape` and `dtype`, and can be added elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums: tensor([[20, 40],\n",
      "        [40, 60],\n",
      "        [60, 80]])\n",
      "Sums shape: torch.Size([3, 2])\n",
      "Sums shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "addend_tensor = torch.tensor([[10, 20], [10, 20], [10, 20]])\n",
    "sums_tensor = temperature_tensor + addend_tensor\n",
    "print(\"Sums:\", sums_tensor)\n",
    "print(\"Sums shape:\", sums_tensor.shape)\n",
    "print(\"Sums shape:\", sums_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797e9f",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "`Linear` from `torch.nn` takes a tensor as input and outputs a tensor whose sizes correspond to `in_features` and `out_features`. The weights and biases involved in the calculation between are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94854a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7611, -0.2199], grad_fn=<ViewBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3318,  0.1210, -0.4353],\n",
      "        [ 0.5348,  0.5282, -0.5684]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4340,  0.2342], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([0.1, -0.1, 0.8])\n",
    "\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdfc5",
   "metadata": {},
   "source": [
    "### Sequential layer\n",
    "`Sequential` from `torch.nn` can stack layers such as `Linear` to pass data through the layers in sequence. Layers bookended by the input and output are called hidden layers.\n",
    "\n",
    "A neuron in a linear layer has $n+1$ parameters, with $n$ counting the weight for each input from the previous layer and $1$ accounting the neuron's bias.\n",
    "\n",
    "More hidden layers = more parameters = higher model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e903681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3289d2",
   "metadata": {},
   "source": [
    "Acquire the model's parameters using `parameters()`, which outputs a container of tensors containing each layer's weights and each layer's biases.\n",
    "\n",
    "`numel()` outputs the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2044, -0.3502,  0.4553],\n",
      "        [ 0.5028, -0.4500, -0.4404]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2631,  0.5386], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2358,  0.3472],\n",
      "        [ 0.5581, -0.5005],\n",
      "        [ 0.6865,  0.6474],\n",
      "        [-0.0606, -0.5850],\n",
      "        [ 0.4166, -0.4181],\n",
      "        [-0.6183,  0.1080],\n",
      "        [-0.1166, -0.1088],\n",
      "        [ 0.1808,  0.6320]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6204, -0.4627,  0.0551,  0.1314, -0.2035,  0.5163, -0.3902,  0.7050],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1125,  0.0053,  0.2274,  0.0758, -0.2285, -0.0329,  0.0310, -0.3375],\n",
      "        [-0.0017,  0.0559,  0.3066,  0.2623,  0.0890,  0.0215,  0.1443,  0.1948],\n",
      "        [ 0.3497,  0.0680,  0.0157,  0.3237,  0.1773,  0.3146,  0.0838, -0.3032]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1649,  0.3178, -0.2053], requires_grad=True)\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in sequential_model.parameters():\n",
    "    print(parameter)\n",
    "    count += parameter.numel()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658294cf",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "Type of function that takes a real-valued input (specifically a float) and outputs a single value between 0 and 1. Used for binary classification, and can be placed as the final activation of a network of linear layers after which a forward pass determines classification-or-not by a threshold (for instance 0.5).\n",
    "\n",
    "Equivalent to traditional logistic regression (in that the output is a probability for the category of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0251], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "sigmoid_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28045",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Type of function that takes a one-dimensional input (specifically of floats) and outputs a one-dimensional distribution of probabilities that sum to 1. Used for multi-class classification, and can be placed as the final activation of a network of linear layers after which a forward pass produces the classification to be chosen from the highest per-class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2d56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.6944e-04, 4.9512e-07, 1.6559e-07, 3.1283e-02, 9.6775e-01],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "softmax_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 5),\n",
    "    nn.Softmax()\n",
    ")\n",
    "softmax_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d25b79",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "Greater function that quantifies how far a machine learning model's predictions are from the actual target values, be it during training or in practice. The loss function takes a model prediction $\\hat{y}$ (may be a singular regressive / sigmoid output, or a softmax tensor of probabilities) and ground truth $y$ (the actual value or class itself) as inputs and outputs a single float, the loss.\n",
    "\n",
    "The goal of training is to minimize the loss, which should be low or zero for an accurate prediction and high for an incorrect one.\n",
    "\n",
    "For cross-entropy loss, the ground truth value may be the class itself (a number), so to convert it into a tensor functional against the model prediction (a softmax probability distribution), use `nn.functional.one_hot()` which takes a tensor of indices to make one-hots for and a num_elements and to output a tensor of containing one-hot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dc10bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(0), 3))\n",
    "print(F.one_hot(torch.tensor(1), 3))\n",
    "print(F.one_hot(torch.tensor([0,2]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e4c9a",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss\n",
    "\n",
    "Cross-entropy loss is a common loss function for classification. With a scores tensor (model predictions before the final softmax function) and a one-hot encoded ground truth label as input (both must be converted to floats), the cross-entropy loss function applies an internal softmax to the scores (producing a probability distribution of the same size), then outputs the negative natural log of the ground truth's corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff28c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4235e-05, 9.7807e-01, 2.1880e-02])\n",
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/Documents/GitHub/ml-playground/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.tensor([-5.2, 4.6, 0.8])\n",
    "one_hot_target = F.one_hot(torch.tensor(0), 3)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "print(softmax(scores))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

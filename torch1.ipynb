{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811915f",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch\n",
    "Tensors are similar to NumPy arrays but have unique features. Convert a Python list into a Torch tensor with `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7af35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temperatures = [[10, 20], [30, 40], [50, 60]]\n",
    "\n",
    "temperature_tensor = torch.tensor(temperatures)\n",
    "print(temperature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1095cd",
   "metadata": {},
   "source": [
    "Tensors have a `shape` and `dtype`, and can be added elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums: tensor([[20, 40],\n",
      "        [40, 60],\n",
      "        [60, 80]])\n",
      "Sums shape: torch.Size([3, 2])\n",
      "Sums shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "addend_tensor = torch.tensor([[10, 20], [10, 20], [10, 20]])\n",
    "sums_tensor = temperature_tensor + addend_tensor\n",
    "print(\"Sums:\", sums_tensor)\n",
    "print(\"Sums shape:\", sums_tensor.shape)\n",
    "print(\"Sums shape:\", sums_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797e9f",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "`Linear` from `torch.nn` takes a tensor as input and outputs a tensor whose sizes correspond to `in_features` and `out_features`. The weights and biases involved in the calculation between are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94854a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7611, -0.2199], grad_fn=<ViewBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3318,  0.1210, -0.4353],\n",
      "        [ 0.5348,  0.5282, -0.5684]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4340,  0.2342], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([0.1, -0.1, 0.8])\n",
    "\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdfc5",
   "metadata": {},
   "source": [
    "### Sequential layer\n",
    "`Sequential` from `torch.nn` can stack layers such as `Linear` to pass data through the layers in sequence. Layers bookended by the input and output are called hidden layers.\n",
    "\n",
    "A neuron in a linear layer has $n+1$ parameters, with $n$ counting the weight for each input from the previous layer and $1$ accounting the neuron's bias.\n",
    "\n",
    "More hidden layers = more parameters = higher model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e903681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3289d2",
   "metadata": {},
   "source": [
    "Acquire the model's parameters using `parameters()`, which outputs a container of tensors containing each layer's weights and each layer's biases.\n",
    "\n",
    "`numel()` outputs the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2044, -0.3502,  0.4553],\n",
      "        [ 0.5028, -0.4500, -0.4404]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2631,  0.5386], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2358,  0.3472],\n",
      "        [ 0.5581, -0.5005],\n",
      "        [ 0.6865,  0.6474],\n",
      "        [-0.0606, -0.5850],\n",
      "        [ 0.4166, -0.4181],\n",
      "        [-0.6183,  0.1080],\n",
      "        [-0.1166, -0.1088],\n",
      "        [ 0.1808,  0.6320]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6204, -0.4627,  0.0551,  0.1314, -0.2035,  0.5163, -0.3902,  0.7050],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1125,  0.0053,  0.2274,  0.0758, -0.2285, -0.0329,  0.0310, -0.3375],\n",
      "        [-0.0017,  0.0559,  0.3066,  0.2623,  0.0890,  0.0215,  0.1443,  0.1948],\n",
      "        [ 0.3497,  0.0680,  0.0157,  0.3237,  0.1773,  0.3146,  0.0838, -0.3032]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1649,  0.3178, -0.2053], requires_grad=True)\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in sequential_model.parameters():\n",
    "    print(parameter)\n",
    "    count += parameter.numel()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658294cf",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "Type of function that takes a real-valued input (specifically a float) and outputs a single value between 0 and 1. Used for binary classification, and can be placed as the final activation of a network of linear layers after which  classification-or-not is done by a threshold (for instance 0.5).\n",
    "\n",
    "Equivalent to traditional logistic regression (in that the output is a probability for the category of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0251], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "sigmoid_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28045",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Type of function that takes a one-dimensional input (specifically of floats) and outputs a one-dimensional array of probabilities summing to 1. Used for multi-class classification, and can be placed as the final activation of a network of linear layers after which the classification is chosen from the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2d56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

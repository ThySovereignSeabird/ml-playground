{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811915f",
   "metadata": {},
   "source": [
    "### Deep Learning with PyTorch\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Tensors are similar to NumPy arrays but have unique features. Convert a Python list into a Torch tensor with `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7af35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temperatures = [[10, 20], [30, 40], [50, 60]]\n",
    "\n",
    "temperature_tensor = torch.tensor(temperatures)\n",
    "print(temperature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1095cd",
   "metadata": {},
   "source": [
    "Tensors have a `shape` and `dtype`, and can be added elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb4353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums: tensor([[20, 40],\n",
      "        [40, 60],\n",
      "        [60, 80]])\n",
      "Sums shape: torch.Size([3, 2])\n",
      "Sums shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "addend_tensor = torch.tensor([[10, 20], [10, 20], [10, 20]])\n",
    "sums_tensor = temperature_tensor + addend_tensor\n",
    "print(\"Sums:\", sums_tensor)\n",
    "print(\"Sums shape:\", sums_tensor.shape)\n",
    "print(\"Sums shape:\", sums_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83797e9f",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "`Linear` from `torch.nn` takes a tensor as input and outputs a tensor whose sizes correspond to `in_features` and `out_features`. The weights and biases involved in the calculation between are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94854a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7825, -0.9039], grad_fn=<ViewBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.1909, -0.4258,  0.5062],\n",
      "        [-0.5135, -0.2065, -0.4223]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3540, -0.5354], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([0.1, -0.1, 0.8])\n",
    "\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdfc5",
   "metadata": {},
   "source": [
    "### Sequential layer\n",
    "`Sequential` from `torch.nn` can stack layers such as `Linear` to pass data through the layers in sequence. Layers bookended by the input and output are called hidden layers.\n",
    "\n",
    "A neuron in a linear layer has $n+1$ parameters, with $n$ counting the weight for each input from the previous layer and $1$ accounting the neuron's bias.\n",
    "\n",
    "More hidden layers = more parameters = higher model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e903681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Linear(8, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3289d2",
   "metadata": {},
   "source": [
    "Acquire the model's parameters using `parameters()`, which outputs a container of tensors containing each layer's weights and each layer's biases.\n",
    "\n",
    "`numel()` outputs the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d2321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4372, -0.3712, -0.4777],\n",
      "        [-0.4745, -0.3590,  0.3441]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5434, 0.4214], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6535, -0.6952],\n",
      "        [ 0.3978, -0.6286],\n",
      "        [ 0.0502,  0.2019],\n",
      "        [ 0.2138,  0.1033],\n",
      "        [ 0.2212,  0.0905],\n",
      "        [ 0.4450, -0.1702],\n",
      "        [-0.3050,  0.5096],\n",
      "        [ 0.1094,  0.5195]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2077,  0.0894, -0.6579,  0.4189,  0.4407,  0.0286, -0.2390,  0.1748],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0654,  0.0603,  0.1492,  0.0654,  0.0913, -0.1673,  0.2176,  0.1341],\n",
      "        [ 0.2890, -0.1768, -0.0975,  0.0136,  0.2267, -0.1219,  0.0688, -0.2123],\n",
      "        [ 0.2221, -0.2042, -0.2308,  0.1172,  0.0818, -0.2777,  0.2426,  0.1920]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1055,  0.3396, -0.1689], requires_grad=True)\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in sequential_model.parameters():\n",
    "    print(parameter)\n",
    "    count += parameter.numel()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658294cf",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "Type of function that takes a real-valued input (specifically a float) and outputs a single value between 0 and 1. Used for binary classification, and can be placed as the final activation of a network of linear layers after which a forward pass determines classification-or-not by a threshold (for instance 0.5).\n",
    "\n",
    "Equivalent to traditional logistic regression (in that the output is a probability for the category of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34d2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5200], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "sigmoid_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28045",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Type of function that takes a one-dimensional input (specifically of floats) and outputs a one-dimensional distribution of probabilities that sum to 1. Used for multi-class classification, and can be placed as the final activation of a network of linear layers after which a forward pass produces the classification to be chosen from the highest per-class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04f2d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/Documents/GitHub/ml-playground/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8374, 0.0024, 0.0745, 0.0015, 0.0842], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([10.0, 12.0, 13.0])\n",
    "softmax_model = nn.Sequential(\n",
    "    nn.Linear(3, 2),\n",
    "    nn.Linear(2, 5),\n",
    "    nn.Softmax()\n",
    ")\n",
    "softmax_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d25b79",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "The function that quantifies how far a machine learning model's predictions are from the actual target values, be it during training or in practice. The loss function takes a model prediction $\\hat{y}$ (may be a singular regressive / sigmoid output, or a softmax tensor of probabilities) and ground truth $y$ (the actual value or class itself) as inputs and outputs a single float, the loss.\n",
    "\n",
    "The goal of training is to minimize the loss, which should be low or zero for an accurate prediction and high for an incorrect one.\n",
    "\n",
    "For cross-entropy loss, the ground truth value may be the class itself (a number), so to convert it into a tensor functional against the model prediction (a softmax probability distribution), use `nn.functional.one_hot()` which takes a tensor of indices to make one-hots for and a num_elements and to output a tensor of containing one-hot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dc10bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(0), 3))\n",
    "print(F.one_hot(torch.tensor(1), 3))\n",
    "print(F.one_hot(torch.tensor([0,2]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e4c9a",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss\n",
    "\n",
    "Cross-entropy loss is a common loss function for classification. With a scores tensor (model predictions before the final softmax function) and a one-hot encoded ground truth label as input (both must be converted to floats), the cross-entropy loss function applies an internal softmax to the scores (producing a probability distribution of the same size), then outputs the negative natural log of the ground truth's corresponding probability.\n",
    "\n",
    "Cross-entropy loss function is supplied in PyTorch by instantiating `nn.CrossEntropyLoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff28c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4235e-05, 9.7807e-01, 2.1880e-02])\n",
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.tensor([-5.2, 4.6, 0.8])\n",
    "one_hot_target = F.one_hot(torch.tensor(0), 3)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "print(softmax(scores))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521686e",
   "metadata": {},
   "source": [
    "#### Loss gradients and Backpropagation\n",
    "\n",
    "Loss outputted by a forward pass may vary by the values used for the model's parameters (weights and biases); this rate of change is the loss gradient. With the output (i.e. `loss = criterion(prediction, target)`) of an instantiated loss function (such as `criterion = CrossEntropyLoss()`), we can calculate the gradients of this loss using `loss.backward()`.\n",
    "\n",
    "Backpropagation is the process by which we aim to locate the global minimum of the loss function and tune the model's parameters to better fit. with each forward pass, we incrementally update the model's weights and biases inversely to the loss gradient such that they follow the gradient downstream (gradient descent).\n",
    "\n",
    "The existing gradients of layer `i` can be accessed using `model[i].weight.grad` and `model[i].bias.grad`. To update model parameters manually, access each layer gradient and multiply it by the learning rate, then subtract the result from the weight or bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45d191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.3206, -0.3639,  0.3761, -0.4230, -0.1317],\n",
      "        [ 0.3998, -0.2565,  0.4259,  0.0580, -0.2682],\n",
      "        [ 0.1348,  0.2798,  0.2490,  0.1747, -0.3247],\n",
      "        [-0.2314, -0.3041,  0.4000,  0.3989, -0.1950],\n",
      "        [-0.2652,  0.2406,  0.3434, -0.1151,  0.1719],\n",
      "        [-0.0882,  0.0454, -0.3510,  0.0415, -0.0767],\n",
      "        [ 0.2033,  0.3179, -0.2007,  0.0304, -0.1262],\n",
      "        [-0.2840, -0.2974, -0.2339,  0.1518,  0.0749]], requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([ 0.1527, -0.3415], requires_grad=True)\n",
      "Gradient of the first layer: tensor([[ 0.0045,  0.0348, -0.0262,  0.0470,  0.0377],\n",
      "        [ 0.0942,  0.7219, -0.5435,  0.9740,  0.7812],\n",
      "        [ 0.0820,  0.6286, -0.4732,  0.8481,  0.6802],\n",
      "        [ 0.0729,  0.5587, -0.4206,  0.7538,  0.6045],\n",
      "        [-0.0388, -0.2971,  0.2237, -0.4008, -0.3215],\n",
      "        [ 0.0782,  0.5995, -0.4513,  0.8088,  0.6487],\n",
      "        [-0.0241, -0.1848,  0.1391, -0.2494, -0.2000],\n",
      "        [ 0.0076,  0.0580, -0.0437,  0.0782,  0.0628]])\n",
      "Gradient of the second layer: tensor([[-1.5181, -1.4358,  0.7204, -1.0607, -0.6195,  1.1818,  1.0049, -0.2656],\n",
      "        [ 1.5181,  1.4358, -0.7204,  1.0607,  0.6195, -1.1818, -1.0049,  0.2656]])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 8),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Need to compute loss and backpropagate in order to define gradients\n",
    "\n",
    "x = torch.randn(5) # dummy input\n",
    "target = torch.randn(2) # dummy target\n",
    "output = model(x) # dummy prediction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "bias1 = model[1].bias\n",
    "print(\"Weight of the first layer:\", weight0)\n",
    "print(\"Bias of the second layer:\", bias1)\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "print(\"Gradient of the first layer:\", grads0)\n",
    "print(\"Gradient of the second layer:\", grads1)\n",
    "\n",
    "lr = 0.001\n",
    "weight0 = weight0 - lr * grads0 # Note that -= does not work, as grad requires being used in an in-place operation\n",
    "weight1 = weight1 - lr * grads1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad353741",
   "metadata": {},
   "source": [
    "#### Optimized gradient descent\n",
    "\n",
    "PyTorch has an optimized gradient descent function, `torch.optim.SGD()`, that uses Stochastic Gradient Descent (which calculates gradients from one or a small subset of training examples, rather than the computationally expensive entire dataset). Takes a model's parameters and a learning rate as input when instantiating; use `optimizer.step()` to perform an update on all of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "251132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "x = torch.randn(5) # dummy input\n",
    "target = torch.randn(2) # dummy target\n",
    "output = model(x) # dummy prediction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f629c",
   "metadata": {},
   "source": [
    "#### DataFrames\n",
    "\n",
    "`pd.read_csv()` from `pandas` reads into a `pandas.DataFrame` from a .csv file.\n",
    "\n",
    "The `DataFrame.iloc` property is used to select slices of the DataFrame through integer-location based indexing. `iloc[:,1:-1]` for instance selects all but the first and last columns (as columns are dimension 1 of the DataFrame).\n",
    "\n",
    "`to_numpy()` of a DataFrame outputs an equivalent numpy array for easier handling with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample:  tensor([0., 1.], dtype=torch.float64)\n",
      "Label sample:  tensor([2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = pd.read_csv(\"dataset.csv\")\n",
    "# target = dataset.iloc[:, -1]\n",
    "# y = target.to_numpy()\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f2fca",
   "metadata": {},
   "source": [
    "#### Data loading and batching\n",
    "\n",
    "`TensorDataset` and `DataLoader` from `torch.utils.data` help manage data loading and batching during training. \n",
    "\n",
    "Instantiate a `DataLoader` with the following parameters:\n",
    "- the `TensorDataset` itself\n",
    "- `batch_size` determines the number of samples included in each iteration\n",
    "- `shuffle` is whether the data order at each epoch (a full pass through the training data) is randomized, which helps improve model generalization (level of performance on unseen data).\n",
    "\n",
    "The `DataLoader` behaves like a PEZ dispenser; each element in it is a tuple unpacked as batch_inputs (features) and batch_labels (target), representing a row from the dataset. Iterating through the `DataLoader` dispenses feature and label tensors that are `batch_size` in depth (aka `batch_size` number of samples, or fewer if there are no more left), corresponding to `batch_size` rows in the `TensorDataset`. \n",
    "\n",
    "In real-world deep learning, batch sizes are often 32 or greater to accommodate larger datasets with better computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9668f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: tensor([0., 1.], dtype=torch.float64)\n",
      "Label sample: tensor([2.], dtype=torch.float64)\n",
      "Batch inputs: tensor([[ 0.,  1.],\n",
      "        [-1.,  0.]], dtype=torch.float64)\n",
      "Batch labels: tensor([[2.],\n",
      "        [4.]], dtype=torch.float64)\n",
      "Batch inputs: tensor([[-1., -1.],\n",
      "        [-1.,  1.]], dtype=torch.float64)\n",
      "Batch labels: tensor([[1.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "Batch inputs: tensor([[1., 0.]], dtype=torch.float64)\n",
      "Batch labels: tensor([[3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X = np.array([[0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [-1.0, 0.0],\n",
    "              [-1.0, 1.0],\n",
    "              [-1.0, -1.0]])\n",
    "y = np.array([[2.0],\n",
    "              [3.0],\n",
    "              [4.0],\n",
    "              [0.0],\n",
    "              [1.0]])\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "\n",
    "input_sample, label_sample = dataset[0]\n",
    "print(\"Input sample:\", input_sample)\n",
    "print(\"Label sample:\", label_sample)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print(\"Batch inputs:\", batch_inputs)\n",
    "    print(\"Batch labels:\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2165c",
   "metadata": {},
   "source": [
    "#### Mean Squared Error loss\n",
    "\n",
    "Mean Squared Error (MSE) loss is a common loss function for regression. With a predictions tensor and ground truth tensor as input (both must be converted to floats), MSE loss is the average of the squared difference between each prediction and its corresponding ground truth.\n",
    "\n",
    "The MSE loss function is implemented in NumPy as the function below, and is supplied in PyTorch by instantiating `nn.MSELoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea8a2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "    return np.mean((prediction - target) ** 2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# loss = criterion(torch.tensor(prediction), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebee68e",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "Training a neural network involves:\n",
    "- creating a model\n",
    "- choosing a loss function\n",
    "- defining a dataset\n",
    "- setting an optimizer (optimized gradient descent function)\n",
    "- and running the training loop (calculating loss via a forward pass, computing gradients via backpropagation, and updating model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop over the number of epochs and then the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader: # data is a tuple with features and target\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get feature and target from the dataloader\n",
    "    feature, target = data\n",
    "\n",
    "    # Run a forward pass\n",
    "    prediction = model(feature)\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    loss = criterion(prediction, target)\n",
    "    loss.backward() # Update gradients\n",
    "\n",
    "    # Descend gradients / update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962a112",
   "metadata": {},
   "source": [
    "#### ReLU activation functions\n",
    "\n",
    "Sometimes activation functions can shrink gradients too much, reducing training efficiency:\n",
    "- Sigmoid and softmax functions bound their outputs between 0 and 1, so have very small gradients for large and small values of $x$ (namely, as the sigmoid output approaches either 0 or 1).\n",
    "- This phenomenon is called saturation, which introduces the vanishing gradients problem (each gradient depends on the previous one during backpropagation, so extremely small gradients can fail to update the weights effectively). This is also what renders sigmoid and softmax suboptimal for hidden layers (use them in the last layer only).\n",
    "\n",
    "Common activation functions designed for use in hidden layers include:\n",
    "- The Rectified Linear Unit (ReLU), defined by the function $f(x) = max(x, 0)$, converts all negative inputs to 0. Its gradients do not approach 0 for large values of $x$, circumventing the vanishing gradients problem. \n",
    "    - ReLU is supplied in PyTorch by instantiating `nn.ReLU()`.\n",
    "- Leaky ReLU is a variation that scales negative inputs by a very small coefficient (default 0.01 in PyTorch), preventing neurons from ceasing learning with zero-value gradients which can occur with standard ReLU.\n",
    "    - Leaky ReLU is supplied in PyTorch by instantiating `nn.LeakyReLU()` with a `negative_slope` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf319120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU applied to positive x: tensor(2.)\n",
      "ReLU applied to negative x: tensor(0.)\n",
      "ReLU applied to positive x: tensor(2.)\n",
      "ReLU applied to negative x: tensor(-0.1500)\n"
     ]
    }
   ],
   "source": [
    "x_pos = torch.tensor(2.0)\n",
    "x_neg = torch.tensor(-3.0)\n",
    "\n",
    "relu_torch = nn.ReLU()\n",
    "print(\"ReLU applied to positive x:\", relu_torch(x_pos))\n",
    "print(\"ReLU applied to negative x:\", relu_torch(x_neg))\n",
    "\n",
    "leakyrelu_torch = nn.LeakyReLU(0.05)\n",
    "print(\"ReLU applied to positive x:\", leakyrelu_torch(x_pos))\n",
    "print(\"ReLU applied to negative x:\", leakyrelu_torch(x_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475888f",
   "metadata": {},
   "source": [
    "#### Learning rate and momentum\n",
    "\n",
    "Recall that training a neural network is optimizing its parameters to achieve minimum loss, and the Stochastic Gradient Descent (SGD) optimizer is what descends the model's gradients to this end.\n",
    "\n",
    "`optim.SGD()` actually takes two key arguments after the model's parameters:\n",
    "- `lr` (the learning rate) controls the step size; larger increases efficiency, smaller increases precision and avoids getting stuck going back-and-forth about minima.\n",
    "    - Typical range: `0.01` to `0.0001`. Rule of thumb: start with `0.001`.\n",
    "- `momentum` adds inertia to help the optimizer avoid getting stuck in local minima; with larger momentum, steep drops in loss push the gradient descent further.\n",
    "    - Typical range: `0.85` to `0.99`. Rule of thumb: start with `0.95`.\n",
    "\n",
    "Momentum is crucial when the global minimum to be found is of a non-convex (i.e. local minima-riddled) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "306f867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e8041",
   "metadata": {},
   "source": [
    "#### Layer initialization\n",
    "\n",
    "The weights of a layer are often initialized to small values; PyTorch's `nn.Linear` for instance initializes its weights between roughly `-0.125` and `0.125`. Keeping both the input data and layer weights small ensures stable training and prevents extreme values that could slow training.\n",
    "\n",
    "`nn.init.uniform_()` takes the `weight` property of a layer such as `nn.Linear` as input and initializes the weights with a uniform distribution between 0 and 1.\n",
    "\n",
    "#### Transfer learning\n",
    "\n",
    "Transfer learning is the reuse of a model trained on a first task for a second similar task. For instance, a model trained on US data scientist salaries can be repurposed as a model to train on European salaries, leveraging the latent information within the original model's weights as a starting point.\n",
    "\n",
    "Save and load weights to and from local files using `torch.save(layer, \"layer.pth\")` and `torch.load(\"layer.pth\")`, which work on any type of PyTorch objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ed10f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(64, 128)\n",
    "\n",
    "# torch.save(layer, \"layer.pth\")\n",
    "# new_layer = torch.load(\"layer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54c674",
   "metadata": {},
   "source": [
    "#### Fine-tuning\n",
    "\n",
    "A type of transfer learning where we load weights from a previously trained model and fine-tune them with a small learning rate and/or with freeze certain parts of the network (often the early ones, whereas the layers closer to the output get fine-tuned).\n",
    "\n",
    "Fine-tuning process:\n",
    "- Find a model trained on a similar task\n",
    "- Load pre-trained weights\n",
    "- Optionally freeze some of the layers in the model\n",
    "- Train with a smaller learning rate\n",
    "- Inspect loss for if learning rate needs to be adjusted\n",
    "\n",
    "Deep learning engineering rarely train their model from scratch as a result of fine-tuning's usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(64, 128),\n",
    "    nn.Linear(128, 64)\n",
    ")\n",
    "\n",
    "# named_parameters returns the name and parameter itself\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"0_weight\":\n",
    "        param.requires_grad = False\n",
    "        # setting this to False ceases gradient tracking,\n",
    "        # which essentially marks these as ignored\n",
    "        # by the optmizer thereby preventing further\n",
    "        # updates to the layer's weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
